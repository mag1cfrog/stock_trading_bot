# 2024-12-18 - Database Storage Optimization: DuckDB Pattern Analysis and SQLite Comparison

## Overview
Today we conducted two significant benchmarking studies to optimize our database storage solution:
1. A comprehensive evaluation of different DuckDB storage patterns, testing various batch sizes and data volumes to determine the optimal approach for our stock data storage system.
2. A comparative analysis between DuckDB and SQLite to validate our database choice, focusing on performance characteristics crucial for our specific use case.

## Part 1: DuckDB Storage Pattern Analysis

### Benchmark Configuration
- **Test Scenarios**: 
  - Heavy load (50 symbols, 500,000 rows)
  - Light load (50 symbols, 5,000 rows)
- **Batch Sizes Tested**: 1 and 100
- **Operations Measured**: Create, Append, Query
- **Test Repetitions**: 5 runs for each scenario
- **Patterns Tested**:
  1. Single Database Pattern (Option 1)
  2. Hierarchical Directory Pattern (Option 2)
  3. Database per Symbol Pattern (Option 3)

### Benchmark Results

#### Heavy Load Results (50 symbols, 500,000 rows)
<table>
  <tr>
    <th rowspan="2">Operation</th>
    <th colspan="3">Batch Size: 100</th>
    <th colspan="3">Batch Size: 1</th>
  </tr>
  <tr>
    <th>Option 1</th>
    <th>Option 2</th>
    <th>Option 3</th>
    <th>Option 1</th>
    <th>Option 2</th>
    <th>Option 3</th>
  </tr>
  <tr>
    <td>Create</td>
    <td>0.55s ± 0.05s</td>
    <td>3.26s ± 0.14s</td>
    <td>1.75s ± 0.01s</td>
    <td>0.57s ± 0.03s</td>
    <td>3.72s ± 0.27s</td>
    <td>1.78s ± 0.11s</td>
  </tr>
  <tr>
    <td>Append</td>
    <td>96.72s ± 0.76s</td>
    <td>142.30s ± 1.49s</td>
    <td>73.11s ± 0.08s</td>
    <td>68.97s ± 10.91s</td>
    <td>40.85s ± 1.83s</td>
    <td>39.13s ± 1.10s</td>
  </tr>
  <tr>
    <td>Query</td>
    <td>0.30s ± 0.00s</td>
    <td>2.96s ± 0.01s</td>
    <td>0.80s ± 0.01s</td>
    <td>0.65s ± 0.42s</td>
    <td>2.60s ± 0.08s</td>
    <td>0.84s ± 0.01s</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>97.57s ± 0.71s</td>
    <td>148.53s ± 1.62s</td>
    <td>75.67s ± 0.10s</td>
    <td>70.18s ± 11.27s</td>
    <td>47.16s ± 1.92s</td>
    <td>41.75s ± 1.17s</td>
  </tr>
</table>

#### Light Load Results (50 symbols, 5,000 rows, Batch Size: 1)
<table>
  <tr>
    <th>Operation</th>
    <th>Option 1</th>
    <th>Option 2</th>
    <th>Option 3</th>
  </tr>
  <tr>
    <td>Create</td>
    <td>0.53s ± 0.03s</td>
    <td>3.34s ± 0.14s</td>
    <td>1.49s ± 0.03s</td>
  </tr>
  <tr>
    <td>Append</td>
    <td>1.72s ± 0.11s</td>
    <td>5.72s ± 0.11s</td>
    <td>1.92s ± 0.04s</td>
  </tr>
  <tr>
    <td>Query</td>
    <td>0.05s ± 0.00s</td>
    <td>0.89s ± 0.02s</td>
    <td>0.27s ± 0.01s</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>2.30s ± 0.14s</td>
    <td>9.95s ± 0.24s</td>
    <td>3.68s ± 0.07s</td>
  </tr>
</table>

### Key Findings

#### Impact of Batch Size
1. **Large Datasets (500,000 rows)**:
   - Batch size 100: Option 3 performs best (75.67s total)
   - Batch size 1: Option 3 remains most efficient (41.75s total)
   - Significant improvement in append operations with batch size 1

2. **Small Datasets (5,000 rows)**:
   - Option 1 shows best performance (2.30s total)
   - Option 3 maintains competitive performance (3.68s total)
   - Option 2 consistently shows highest overhead

#### Pattern-Specific Analysis
1. **Option 1** (Single Database Pattern):
   - One database file containing all data
   - Tables organized by symbol and timeframe combinations
   - Best for small datasets
   - Performance degrades with larger datasets
   - High variance in append operations

2. **Option 2** (Hierarchical Directory Pattern):
   - Data distributed in hierarchical structure: symbol/timeframe/database
   - Each timeframe gets its own database file
   - Consistently highest overhead due to filesystem operations
   - Poor scaling with dataset size
   - Not recommended due to excessive file handling

3. **Option 3** (Database per Symbol Pattern):
   - One database file per symbol
   - Each database contains tables for different timeframes
   - Most consistent performance across scenarios
   - Best scaling with large datasets
   - Good balance between organization and performance

## Part 2: DuckDB vs SQLite Comparison

### Benchmark Configuration
- **Dataset**: Single table with 100,000 rows
- **Operations Tested**: Create, Insert, Query
- **Test Repetitions**: 5 runs
- **Databases Compared**: 
  1. DuckDB (latest version)
  2. SQLite (version 3.x)

### Benchmark Results
<table>
  <tr>
    <th>Operation</th>
    <th>DuckDB</th>
    <th>SQLite</th>
    <th>Performance Difference</th>
  </tr>
  <tr>
    <td>Create</td>
    <td>0.002s</td>
    <td>0.003s</td>
    <td>1.5x faster</td>
  </tr>
  <tr>
    <td>Insert</td>
    <td>0.027s</td>
    <td>0.624s</td>
    <td>23.1x faster</td>
  </tr>
  <tr>
    <td>Query</td>
    <td>0.003s</td>
    <td>0.034s</td>
    <td>11.3x faster</td>
  </tr>
</table>

### Key Findings
1. **Overall Performance**: DuckDB significantly outperforms SQLite in our specific use case
2. **Insert Operations**: Most notable difference in insert performance, with DuckDB being ~23x faster
3. **Query Efficiency**: DuckDB demonstrates ~11x faster query execution
4. **Transactional Workloads**: Despite SQLite's reputation for transaction handling, DuckDB proves more efficient for our data patterns

This comparison reinforces our decision to use DuckDB as our primary storage engine, particularly given our focus on high-performance data ingestion and query operations.

## Final Decision and Implementation Plan

### Selected Approach: Option 3
We will implement Option 3 (Batch processing with optimized transactions) as our standard pattern because:
1. Best overall performance for large datasets
2. Consistent performance across different scenarios
3. Good balance between complexity and efficiency
4. Better error handling capabilities
